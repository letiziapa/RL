{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eabe8161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f09574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space:\n",
      "0- cart position\n",
      "1- cart velocity\n",
      "2- pole angle (radians)\n",
      "3- pole angular velocity\n",
      "low bounds: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], \n",
      "high bounds: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], \n",
      "shape: (4,), \n",
      "type: float32\n",
      "\n",
      "Action space: 2\n",
      "0- push cart to the left\n",
      "1- push cart to the right\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "#environment characteristics\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(f\"\"\"Observation space:\n",
    "0- cart position\n",
    "1- cart velocity\n",
    "2- pole angle (radians)\n",
    "3- pole angular velocity\n",
    "low bounds: {observation_space.low}, \n",
    "high bounds: {observation_space.high}, \n",
    "shape: {observation_space.shape}, \n",
    "type: {observation_space.dtype}\"\"\")\n",
    "\n",
    "print(f\"\"\"\\nAction space: {action_space.n}\n",
    "0- push cart to the left\n",
    "1- push cart to the right\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779cd93d",
   "metadata": {},
   "source": [
    "Conditions for termination: angle over 12 degrees (unstable), cart position more than 2.4 (out of frame), episode length > 200, or solved requirement (avg return is greater than or equal to 195 over 100 consecutive trials).\n",
    "\n",
    "When we call env.step we get different outputs: observation tells us the state of the environment (current cart position on x, cart velocity, pole angle and pole angular velocity); the reward achieved (1 for every step taken, 0 after termination), info is used for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c926b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the agents\n",
    "class RandomAgent:\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self):\n",
    "        #randomly select an action\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "class LearningAgent:\n",
    "    def __init__(self, env, alpha = 0.1, epsilon = 0.9, gamma = 0.99, episodes = 1000, is_random = False, total_episodes_trained = 0, render = False):\n",
    "        \n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        #self.bins = bins\n",
    "        self.episodes = episodes\n",
    "        self.epsilon_decay = epsilon / episodes\n",
    "        self.q_table = self._create_q_table()\n",
    "        self.render = render\n",
    "        self.is_random = is_random\n",
    "        self.total_episodes_trained = total_episodes_trained\n",
    "\n",
    "    def digitize_state(self, state):\n",
    "        position_bins = np.linspace(-2.4, 2.4, 10)\n",
    "        velocity_bins = np.linspace(-4, 4, 10)\n",
    "        angle_bins = np.linspace(-0.2095, 0.2095, 10)\n",
    "        angular_velocity_bins = np.linspace(-4, 4, 10)\n",
    "        # Discretize the state using the bins\n",
    "\n",
    "        new_position = np.digitize(state[0], position_bins)\n",
    "        new_velocity = np.digitize(state[1], velocity_bins)\n",
    "        new_angle = np.digitize(state[2], angle_bins)\n",
    "        new_angular_velocity = np.digitize(state[3], angular_velocity_bins)\n",
    "        # Return the discretized state as a tuple\n",
    "        new_state_digitized = [new_position, new_velocity, new_angle, new_angular_velocity]\n",
    "        return new_state_digitized\n",
    "    \n",
    "    def _create_q_table(self):\n",
    "        position_bins = np.linspace(-2.4, 2.4, 10)\n",
    "        velocity_bins = np.linspace(-4, 4, 10)\n",
    "        angle_bins = np.linspace(-0.2095, 0.2095, 10)\n",
    "        angular_velocity_bins = np.linspace(-4, 4, 10)\n",
    "        return np.zeros(\n",
    "            (\n",
    "                len(position_bins) + 1,\n",
    "                len(velocity_bins) + 1,\n",
    "                len(angle_bins) + 1,\n",
    "                len(angular_velocity_bins) + 1,\n",
    "                self.env.action_space.n,\n",
    "            )\n",
    "        )\n",
    "    def act(self, state):\n",
    "        \"\"\"Selects an action based on the epsilon-greedy policy.\"\"\"\n",
    "        discrete_state = self.digitize_state(state)\n",
    " \n",
    "        if self.is_random == True or np.random.uniform(0,1) < self.epsilon:\n",
    "            #action = self.env.action_space.sample()\n",
    "            action = np.random.randint(0, 2)\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[state[0], state[1], state[2], state[3], :])\n",
    "       # print(f\"Action: {action}\")\n",
    "        return action\n",
    "    \n",
    "    def train(self):\n",
    "        cumulative_reward = []\n",
    "        for episode in range(self.episodes):\n",
    "            ep_reward = 0 #initialize episode reward counter\n",
    "            state, _ = self.env.reset()\n",
    "            #state = self.digitize_state(self.env.reset()[0])\n",
    "            state = self.digitize_state(state)\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ , _ = self.env.step(action)\n",
    "                next_state = self.digitize_state(next_state)\n",
    "                ep_reward += reward\n",
    "                #find the max q-value for the next state\n",
    "                #max next q-value is found from the maximum of all possible actions\n",
    "                #this is the agents estimate of the best possible future reward\n",
    "                #for the next state\n",
    "                max_next_q = np.max(self.q_table[\n",
    "                    next_state[0],\n",
    "                    next_state[1],\n",
    "                    next_state[2],\n",
    "                    next_state[3],\n",
    "                    :,\n",
    "                ])\n",
    "                #update the q-table using the q-learning update rule\n",
    "                #q(s,a) = q(s,a) + alpha * (r + gamma * max(q(s',a')) - q(s,a))\n",
    "                self.q_table[\n",
    "                    state[0],\n",
    "                    state[1],\n",
    "                    state[2],\n",
    "                    state[3],\n",
    "                    action,\n",
    "                ] += self.alpha * (reward + self.gamma * max_next_q - self.q_table[\n",
    "                    state[0],\n",
    "                    state[1],\n",
    "                    state[2],\n",
    "                    state[3],\n",
    "                    action,\n",
    "                ])\n",
    "                ep_reward += reward\n",
    "                state = next_state\n",
    "\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "            cumulative_reward.append(ep_reward)\n",
    "            mean_reward = np.mean(cumulative_reward[-100:])\n",
    "\n",
    "            if episode % 100 == 0:\n",
    "                print(\n",
    "                    f\"Episode: {episode + self.total_episodes_trained} Epsilon: {self.epsilon:0.2f}  Mean Rewards {mean_reward:0.1f}\"\n",
    "                )\n",
    "\n",
    "            if mean_reward >= 195:\n",
    "                print(f\"Mean rewards: {mean_reward} - no need to train model longer\")\n",
    "                break\n",
    "        self.env.close()\n",
    "        \n",
    "    def test(self):\n",
    "        \"\"\"Tests the agent in the environment.\"\"\"\n",
    "        #store the rewards for each episode\n",
    "        cumulative_reward = []\n",
    "        for episode in range(self.episodes):\n",
    "            ep_reward = 0\n",
    "            state, _ = self.env.reset()\n",
    "           # state = self.digitize_state(self.env.reset()[0])\n",
    "            state = self.digitize_state(state)\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ , _ = self.env.step(action)\n",
    "                next_state = self.digitize_state(next_state)\n",
    "                ep_reward += reward\n",
    "                state = next_state\n",
    "            #print(f\"Episode: {episode}, Reward: {ep_reward}\")\n",
    "            #create a plot of the rewards\n",
    "            cumulative_reward.append(ep_reward)\n",
    "            mean_reward = np.mean(cumulative_reward[-100:])\n",
    "            if episode % 100 == 0:\n",
    "                print(\n",
    "                    f\"Episode: {episode + self.total_episodes_trained} Mean Reward: {mean_reward:0.1f}\"\n",
    "                )\n",
    "\n",
    "        #close the environment\n",
    "        self.env.close()\n",
    "\n",
    "       \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776fb295",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = LearningAgent(env, alpha=0.05, epsilon=0.99, gamma=0.99, episodes=1000, render = True)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e007b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
